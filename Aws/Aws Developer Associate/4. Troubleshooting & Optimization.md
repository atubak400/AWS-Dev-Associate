

### 1. Using EventBridge for Event-Driven Automation

- If you need to trigger a Lambda function based on specific CodeCommit events, such as pullRequestCreated or pullRequestSourceBranchUpdated, use EventBridge. It provides a direct and efficient mechanism for detecting events and triggering the appropriate Lambda function, reducing complexity and ensuring timely execution.

### 2. Using CloudWatch Custom Metrics for Error Monitoring

- If you need to monitor the error rate of external API calls and alert the support team when it exceeds a threshold, use custom metrics in Amazon CloudWatch. Track failures, configure alarms, and notify the team via Amazon SNS when conditions are met.

### 3. Using Pre-Baked AMIs for Faster EC2 Scaling

- To improve the provisioning time of EC2 instances in an Auto Scaling group, use EC2 Image Builder to create pre-baked AMIs that include the latest application code, dependencies, and patches. This approach ensures faster scaling and minimizes downtime during peak loads. Avoid relying solely on UserData scripts or manual deployment methods, as they increase provisioning time and complexity.

- EC2 UserData scripts are scripts or commands that are provided to an Amazon EC2 instance at the time of launch. These scripts are executed by the instance during its first boot to automate tasks such as installing software, configuring the instance, or starting specific applications.

### 4. Scaling and Optimizing Kinesis for Increased Traffic

- If your Kinesis stream encounters throughput exceptions, reshard your stream to scale capacity and reduce the frequency or size of your requests to optimize producer behavior. These strategies address write-side limitations effectively and help your stream handle increased traffic efficiently. 

- Resharding your stream to scale capacity means increasing the number of shards in your Kinesis stream to handle higher data throughput by distributing the load across more shards

- Reducing the frequency or size of your requests optimizes producer behavior by sending smaller or fewer data chunks, minimizing the risk of exceeding the stream's write limits.

### 5. Efficient Log Analysis with CloudWatch Logs Insights

- For analyzing recent log data in CloudWatch Logs and identifying the most expensive Lambda requests, use CloudWatch Logs Insights. It provides a simple, interactive querying experience directly in the CloudWatch console

- CloudWatch Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. It automatically discovers fields in logs from AWS services, such as Route 53, Lambda, CloudTrail, VPC Flow Logs, and any application or custom log that emits log events as JSON.

### 6. Optimizing DynamoDB Scans During Heavy Loads

- Run parallel scans to process table segments concurrently, significantly reducing the total scan time.
- Reduce the page size to improve request processing times and manage read capacity unit usage more effectively. These methods directly align with AWS's best practices for handling high scan loads.
- DynamoDB uses eventually consistent reads by default. Eventually consistent reads allow you to read data quickly, but it may not reflect the most recent updates immediately, syncing to the latest version over time.

### 7. Automating Notifications for Build Failures in Continuous Integration

- To set up continuous integration with CodeBuild and ensure the DevOps team is notified of build failures:
    - Store the source code in CodeCommit to integrate the latest code seamlessly into the build process. CodeCommit is a supported source code repository for CodeBuild.
    - Use Amazon EventBridge to capture build events and integrate it with Amazon SNS to send SMS notifications to the team. This ensures timely alerts and efficient monitoring of build processes.

### 8. Troubleshooting API Gateway Timeouts
- IntegrationLatency metric measures the time taken by API Gateway to send the request to the backend Lambda function(for example) and receive a response.
- Latency metric measures the total time taken by API Gateway to process a request, including both the request processing time and the backend integration time. A high latency value indicates that the overall API request-response cycle is taking too long, potentially causing the timeouts.
- When troubleshooting API Gateway timeouts, check the IntegrationLatency metric to identify delays in communicating with the backend Lambda function and the Latency metric to evaluate the overall request processing time.

### 9. DAX for High Read Traffic Optimization

- When experiencing high read traffic in DynamoDB, configuring a DAX cluster is the most effective and immediate solution. It enhances performance by reducing latency through caching, ensuring smooth operation during high-demand periods without requiring structural changes to the table.

### 10. ProvisionedThroughputExceededException error
- The ProvisionedThroughputExceededException error in DynamoDB occurs when the number of requests to a table or a global secondary index exceeds the provisioned read or write capacity. This error is a signal that the demand for the table is higher than the capacity allocated for it.

### 11. Handling UnprocessedKeys in DynamoDB BatchGetItem Responses

- To handle unprocessed keys in BatchGetItem responses, leverage the AWS SDK for JavaScript (or other SDKs) to automatically implement retries with exponential backoff. This ensures efficient error handling and maximizes the success rate of batch operations without additional manual intervention. If using custom logic, always implement retries with exponential backoff to handle transient errors effectively.

### 12. Exponential Backoffs

- Exponential backoff is a way to manage retries when something temporarily fails, like a network issue or an overloaded server. Instead of retrying immediately or at fixed intervals, you wait a little longer each time before trying again.

- For example, after the first failure, you might wait 1 second, then 2 seconds after the next failure, then 4 seconds, and so on. This gives the system or service you're trying to reach some time to recover while avoiding adding more strain by retrying too often. 

- It's like giving someone extra time to catch their breath after each failed attempt to answer a question. This approach helps reduce congestion and increases the chances that your retries will succeed.

### 13. AWS SDK Exponential Backoffs

- In the context of AWS SDKs for reliable batch operations, exponential backoff is a strategy used to handle situations where some requests fail temporarily, such as when the system is throttled or experiences transient errors. For example, when you use BatchGetItem in DynamoDB, the response may include unprocessed keys if the service couldn't handle some of the requests due to throughput limits or other temporary conditions.

- AWS SDKs automatically retry these unprocessed requests, applying exponential backoff to manage the retries. This means that after each failed attempt, the SDK waits progressively longer—starting with a short delay and doubling it with each subsequent retry. This approach gives DynamoDB time to recover, avoids overloading the service, and ensures that the unprocessed keys are eventually retrieved without manual intervention.

### 14. Using AWS SDKs for Reliable Batch Operations

- To handle unprocessed keys in BatchGetItem responses, leverage the AWS SDK for JavaScript (or other SDKs) to automatically implement retries with exponential backoff. This ensures efficient error handling and maximizes the success rate of batch operations without additional manual intervention.

- If using custom logic, always implement retries with exponential backoff to handle transient errors effectively.

### 15. AWS Serverless Application Repository (SAR)

- The AWS Serverless Application Repository (SAR) is a service where you can find and share ready-made serverless applications. These are pre-built solutions designed to run on AWS services like Lambda, so you don't have to build everything from scratch. 

- You can browse a collection of apps for tasks like data processing, monitoring, or machine learning, then quickly deploy them to your AWS account. 

### 16. Leveraging Pre-Built Serverless Applications with SAR

- For a company built on AWS serverless architecture looking to accelerate development with pre-built solutions, the AWS Serverless Application Repository (SAR) is the best service. It provides a library of serverless applications, enabling rapid deployment and customization to meet new business requirements.

### 17. ALB access logs

- ALB access logs are detailed records of all the requests handled by your Application Load Balancer (ALB) in AWS. These logs include useful information like the time of the request, the IP address of the client, the requested URL, the response code, and more. They are stored in an Amazon S3 bucket and can help you troubleshoot issues, monitor traffic, and analyze usage patterns.

### 18. Analyzing Request Patterns with ALB Access Logs

- The feature on the Load Balancer that helps analyze incoming requests for latencies and client IP address patterns is the ALB access logs. These logs provide the granular details needed to understand traffic patterns and optimize application performance.

### 19. Handling Periodic Spikes in Kinesis Streams

- When encountering periodic spikes that lead to ProvisionedThroughputExceededException:

    - Implement exponential backoff for retries to handle temporary capacity issues gracefully.
    - Decrease the frequency or size of your requests to avoid overwhelming the shards.

### 20. X-Ray sampling

- X-Ray sampling is a mechanism used to control the amount of data that X-Ray collects to reduce overhead, particularly in high-traffic applications. While it helps manage the volume of tracing data, it does not help with debugging issues related to why the application fails to send data to X-Ray.

### 21. Understanding Debugging Tools for X-Ray Issues

- When an application fails to send data to X-Ray, ensure:

    - The EC2 instance role is correctly configured with the necessary permissions.
    - The EC2 X-Ray Daemon is running and properly configured.
    - Use CloudTrail to monitor API activity for any potential errors.
    - X-Ray sampling does not provide insights for debugging connectivity or configuration issues.

### 22. Auditing SSM Parameter Store Access

- To generate a report for auditing SSM Parameter Store activity:

    - Use AWS CloudTrail, which tracks all API calls to SSM Parameter Store.
    - Configure CloudTrail to store logs in S3 or send them to CloudWatch Logs for further analysis if required.

### 23. Using X-Ray for Cross-Account Distributed Tracing

- AWS X-Ray helps you track and debug issues in applications that span multiple AWS accounts. It collects detailed trace data from different parts of your application, even if they are running in separate accounts. 

- This makes it easier to see how requests flow through your entire system, find bottlenecks, and fix errors. With its centralized view, X-Ray gives teams a clear picture of performance across all accounts, making debugging and monitoring distributed applications much simpler.

- For debugging and tracing across multiple AWS accounts, use AWS X-Ray. It provides detailed trace information and centralized visualization, helping teams identify performance issues and errors in distributed applications.

### 24. Optimize API Performance with Caching
- If your API serves frequently requested, daily updated data, enabling API caching in API Gateway is the most straightforward and effective way to improve performance by reducing latency and backend load.

### 25. API Gateway Mapping Templates

- API Gateway Mapping Templates are tools that let you change the way data is sent to or received from your APIs without changing the backend. 
- They use a scripting language called Velocity Template Language (VTL) to transform requests or responses. For example, you can reformat data, hide certain fields, or convert it into a different structure before sending it to the client or backend. 
- This helps you customize how your API works without modifying your backend logic, making it easier to adapt to different use cases.

### 26. Ensuring Proper Configuration of Health Checks in ALB

- If EC2 instances are marked as unhealthy by an ALB, check:

    - Security Group Rules: Ensure the ALB's security group can communicate with the EC2 instances on the health check port.
    - Health Check Route: Verify the health check path is correctly configured and returns a 2xx status code. A 2xx response is an HTTP status code category that indicates a successful request. It means the client's request was received, understood, and processed correctly by the server. e.g. 200, 201, 202, 204.

### 27. Optimizing AWS Lambda for CPU-Intensive Workloads

- To make AWS Lambda faster for CPU-heavy tasks, you can increase its memory allocation. When you do this, AWS automatically gives the function more CPU power as well, helping it process tasks quicker. Don't waste time adjusting unrelated settings, like the AWS Region or Lambda layers, because these won't directly improve how fast the function runs. Focusing on memory is the most effective way to boost performance for CPU-intensive workloads.


### 28. Backlog per instance metric and Target tracking scaling policy

- The backlog per instance metric shows how many tasks or messages are waiting to be processed by each running ECS instance. It helps measure workload pressure. - When combined with a target tracking scaling policy, you can automatically adjust the number of ECS tasks to keep the backlog at a manageable level. For example, if the backlog grows because of high traffic, more tasks are launched to process it faster, and when traffic slows down, the tasks scale back down. This approach keeps your application responsive while controlling costs.

### 29. Optimizing ECS with SQS for Variable Workloads

- To handle variable traffic spikes and improve order processing performance:

    - Use the backlog per instance metric to monitor the workload.
    - Combine it with a target tracking scaling policy to dynamically scale ECS tasks based on SQS queue depth.

### 30. Understanding EC2 User Data Behavior

- By default, user data scripts run with root privileges, enabling administrative setup tasks during instance initialization.
- These scripts execute only during the first boot cycle.

### 31. Policy Statement

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowForUseOfThisKey",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::111122223333:role/UserRole"
            },
            "Action": [
                "kms:GenerateDataKeyWithoutPlaintext",
                "kms:Decrypt"
            ],
            "Resource": "*"
        }
    ]
}

- a policy statement is a part of a policy document that defines specific permissions. Each statement includes details like:

    - Effect: Whether the action is allowed or denied (in this case, Allow).
    - Principal: Who gets the permissions (here, the UserRole).
    - Action: What actions are allowed (like kms:GenerateDataKeyWithoutPlaintext and kms:Decrypt).
    - Resource: Where the actions are allowed (here, all resources with *).

- This policy allows the UserRole (a specific role in AWS, i.e arn:aws:iam::111122223333:role/UserRole) to use a KMS key for two tasks: generating encrypted data keys and decrypting data. These permissions apply to all resources, meaning the role can perform these actions wherever the key is used. This setup is commonly used to enable secure data encryption and decryption processes in AWS applications.

### 32. Target Tracking Scaling Policies in EC2 Auto Scaling

- Target Tracking Scaling Policies in EC2 Auto Scaling work like a thermostat for your servers. You set a target, like keeping CPU usage at 50%, and Auto Scaling automatically adjusts the number of servers to stay at that level. 
- If usage goes above 50%, it adds more servers; if it goes below, it removes servers. This ensures your application runs smoothly without overusing or underusing resources.

### 33. Target Tracking Metrics
- ASGAverageNetworkOut: This tracks how much data your Auto Scaling Group is sending out (uploading) over the network. It helps you scale servers if the outgoing traffic becomes too high.

- ALBRequestCountPerTarget: This measures how many requests each server (target) behind your Application Load Balancer is handling. If the number of requests per server gets too high, it can add more servers to balance the load.

- ASGAverageCPUUtilization: This measures the average CPU usage across all servers in your Auto Scaling Group. If the CPU usage gets too high (e.g., 80%), it can add more servers to keep things running smoothly.

- ApproximateNumberOfMessagesVisible (NOT Supported): This metric applies to Amazon SQS queues, measuring the number of messages available in a queue for processing. While it is used for scaling consumers of an SQS queue (e.g., Lambda or EC2 instances), it is not supported directly as a Target Tracking Scaling Policy metric in EC2 Auto Scaling groups.

### 34. Internet Gateway
- An Internet Gateway in AWS is like a bridge that connects your private network (inside AWS) to the public internet. It allows resources in your Virtual Private Cloud (VPC), like servers, to send and receive data over the internet. Without an Internet Gateway, your resources can't directly access or be accessed from the internet.

### 35. Route table

- A route table in the instance's subnet is like a map that tells your instance (a virtual server) where to send data. It decides how the instance communicates with other parts of your network or the internet. For example, it might say, "Send local traffic to other servers in the subnet" or "Send internet traffic through the Internet Gateway." It's essential for directing data to the right place.

### 36. Network ACLs (Access Control Lists
- Network ACLs (Access Control Lists) for a subnet are like security guards for your network. They control what kind of data is allowed in or out of the subnet (a section of your network). You can set rules to allow or block specific types of traffic, like saying "Let in web traffic" or "Block unknown requests." These rules help protect your subnet and its resources.

### 37. Conditions for Internet Connectivity via an Internet Gateway

For an EC2 instance to connect to the Internet via an Internet Gateway:
- The route table in the instance's subnet should have a route to an Internet Gateway.
- The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic.

### 38. Lambda function's execution role

- A Lambda function's execution role is like an ID card that gives the function permission to do specific tasks. For example, if your Lambda function needs to read from a database or write to a storage bucket, the execution role tells AWS, "This function is allowed to do that." It's a way to control what the function can and cannot do securely.

- To track and see what your Lambda function is doing using X-Ray, you need to give it permission to share its activity. This is like allowing the function to send updates about what it's doing to X-Ray. You do this by adding a special permission called AWSXRayDaemonWriteAccess to the function's execution role. If you don't do this, the function won't share its activity, and the X-Ray tool will stay empty.

### 39. Using VPC Flow Logs to Troubleshoot Subnet Traffic Issues

- To check if traffic is reaching a subnet in your VPC, you can use VPC Flow Logs because they track IP traffic going to and from the network interfaces of your VPC resources. These logs help you see details like where the traffic is coming from, where it's going, and whether it is accepted or rejected, making them useful for troubleshooting connectivity issues between subnets or resources.

### 40. AWS CLI --dry-run

- The --dry-run option in AWS CLI is like asking, "Can I do this?" without actually doing it. For example, if you want to check if a user can delete a server (EC2 instance), you run the command with --dry-run to test their permissions. It doesn't make any changes, costs nothing, and is a safe way to check what actions the user is allowed to do.

### 41. Rest Api

- A REST API (Representational State Transfer API) is a way for different software systems to communicate over the internet using standard web protocols like HTTP. It allows clients (like apps or websites) to send requests to a server to get, update, create, or delete data. 
- REST APIs use simple URLs to define resources and common HTTP methods like GET (read data), POST (create data), PUT (update data), and DELETE (remove data).
- REST APIs with Serverless Architecture mean using REST APIs to interact with backend services that run without needing to manage servers. In serverless setups, you use services like AWS Lambda or Azure Functions, which automatically handle the execution of your code when an API request is made.

### 42. Building REST APIs with Serverless Architecture in AWS

- To adopt a serverless architecture for REST APIs, use API Gateway to expose AWS Lambda functionality. This approach eliminates infrastructure management, ensures scalability, and is cost-efficient, making it the best choice for serverless applications.

### 43. Amazon SQS Scalability

- Amazon SQS queues can store an unlimited number of messages, making them ideal for systems that process large volumes of data. 
- However, consider factors such as message retention, size limits, and cost when designing high-throughput applications.

### 44. NACLs and EC2

- When setting up access for an EC2 instance, you need to make sure data can flow both ways. NACLs (Network ACLs) act like Subnet traffic guards and need rules to allow incoming requests and outgoing replies. 
- On the other hand, security groups are smarter and automatically allow replies for any incoming traffic you already permitted into the EC2. This setup ensures your EC2 instance can send and receive data properly with clients on the internet.

### 45. Amazon Cognito User Pools

- Amazon Cognito User Pools are fully managed, scalable, and specifically designed for handling user authentication, sign-up, and sign-in processes, reducing the need for custom coding and infrastructure management.

### 46. Auto Scaling Group Replacement of Unhealthy Instances

When an EC2 instance in an Auto Scaling group is marked as unhealthy:

- The ASG terminates the instance and launches a replacement to maintain the desired capacity.
- This ensures high availability and reliability, with minimal manual intervention.

### 47. Improving Latency for Static Content Delivery

To reduce latency for static content on a website:

- Move static content from EC2 instances to Amazon S3 for better scalability and cost-efficiency.
- Use Amazon CloudFront to cache and distribute static content globally, ensuring faster delivery to users worldwide. This combination offloads work from EC2 and optimizes user experience.

### 48. Kinesis ProvisionedThroughputExceeded

- A ProvisionedThroughputExceeded error in Kinesis happens when too much data is sent to the stream too quickly. 
- To fix this, you can add more shards to increase the stream's capacity, allowing it to handle more data. 
- Also, enable exponential backoff retries in your producer, which makes it wait longer between retries during high traffic, reducing the chance of overwhelming the stream.

### 49. Custom metrics

- Custom metrics are measurements you define to track specific things about your application that AWS doesn't monitor by default. For example, you might want to track how often an API call succeeds or fails, how long a specific process takes, or how many items a user uploads.
- To keep track of errors in a serverless app, you can create custom metrics in CloudWatch to monitor how often an external API succeeds or fails. Set a CloudWatch alarm to alert your team using SNS (like a text or email) if the error rate gets too high. This setup helps you quickly spot and fix problems, making sure your app runs smoothly.

### 50. Handling Throttling in Amazon SES

- When encountering throttling errors in SES, Implement Exponential Backoff to manage retries and distribute the load over time.

### 51. Pre-Signed URLs for Time-Bound Access:

- By making the S3 bucket private, public access is blocked.
- Pre-signed URLs allow you to grant temporary access to objects in a private S3 bucket.
- These URLs are time-bound, meaning the access is only valid for a specific period. Once the time expires, the URL can no longer be used to access the object.

### 52. ValidateService Lifecycle Hook:
- The ValidateService lifecycle event is the final hook in the AWS CodeDeploy deployment process.
- It is used to verify that the deployed application is working as expected and that the deployment was successful.

### 53. Dead-Letter Queue (DLQ):

- A Dead-Letter Queue (DLQ) is a secondary queue that stores messages that could not be successfully processed by the application after a specified number of attempts.
- Configuring a DLQ for the SQS queue ensures that unprocessed or errored messages are isolated for further analysis and troubleshooting without disrupting the main queue.

### 54. VPC Endpoints for DynamoDB:

- VPC endpoints allow Amazon EC2 instances in a VPC to communicate with AWS services like DynamoDB privately without routing traffic through the public internet.
- VPC endpoints use the AWS PrivateLink service to establish private connectivity within the AWS network.

### 55. Provisioned Concurrency
- Provisioned Concurrency in AWS Lambda keeps a specific number of function instances ready to handle requests at all times. Normally, when a Lambda function is called, there’s a small delay (called a cold start) if the function needs to be initialized first. 
- With Provisioned Concurrency, the function is pre-warmed, so it can handle incoming requests immediately, without this delay. This is especially useful for applications with high or unpredictable traffic, where fast response times are critical.

### 56. Lambda Auto Scaling and Provisioned Concurrency
- To keep Lambda functions fast during busy times, like a big sale, you can use Provisioned Concurrency, which pre-starts Lambda instances so they’re ready to handle requests without delay. 
- You can also set up Auto Scaling to adjust the number of pre-started instances automatically based on traffic or a set schedule. 
- This reduces delays (cold starts) and ensures your app runs smoothly even when many people use it at once.

### 57. Improving Deployment Performance with AWS CodePipeline

- To make Elastic Beanstalk deployments faster, package all the needed dependencies during the build stage in AWS CodeBuild. 
- This way, the application is fully prepared before deployment, and the target instances don’t have to download or process dependencies during deployment. 
- This method simplifies the process and speeds up deployments, especially when dealing with many instances.

### 58. Avoiding Log Loops in S3 Access Logging

- When you turn on access logging for an S3 bucket, make sure the logs are saved in a different bucket. If you save the logs in the same bucket you're logging, it can create a loop where new logs are created for every logging event, causing endless growth. 
- Using a separate bucket for logs prevents this problem and keeps things organized.

### 59. Cloudformation Parameters

- Parameters in AWS CloudFormation are placeholders that allow you to input custom values when creating or updating a stack. 
- They make templates more flexible and reusable by letting you define values like instance types, key pair names, or VPC IDs at runtime instead of hardcoding them in the template. 
- Parameters can have types (like String, Number, or AWS-specific types)

### 60. Valid CloudFormation Parameter Types:

- String: A general-purpose parameter type for text-based input. It is the most commonly used parameter type.
- CommaDelimitedList: Accepts a comma-separated list of values (e.g., "value1,value2,value3").
- AWS::EC2::KeyPair::KeyName: A predefined AWS-specific parameter type used to validate EC2 key pair names.

### 61. AWS::EC2::KeyPair::KeyName parameter

- The AWS::EC2::KeyPair::KeyName parameter type in AWS CloudFormation is used to reference the name of an existing Amazon EC2 Key Pair. This Key Pair is used for secure access to EC2 instances via SSH. 
- When you specify this parameter type in your template, CloudFormation provides a dropdown of available Key Pair names in the region during stack creation, ensuring you select a valid Key Pair.

