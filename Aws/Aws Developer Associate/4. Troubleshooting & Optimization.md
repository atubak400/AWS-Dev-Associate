### 1. Using EventBridge for Event-Driven Automation

- If you need to trigger a Lambda function based on specific CodeCommit events, such as pullRequestCreated or pullRequestSourceBranchUpdated, use EventBridge. It provides a direct and efficient mechanism for detecting events and triggering the appropriate Lambda function, reducing complexity and ensuring timely execution.

### 2. Using CloudWatch Custom Metrics for Error Monitoring

- If you need to monitor the error rate of external API calls and alert the support team when it exceeds a threshold, use custom metrics in Amazon CloudWatch. Track failures, configure alarms, and notify the team via Amazon SNS when conditions are met.

### 3. Using Pre-Baked AMIs for Faster EC2 Scaling

- To improve the provisioning time of EC2 instances in an Auto Scaling group, use EC2 Image Builder to create pre-baked AMIs that include the latest application code, dependencies, and patches. This approach ensures faster scaling and minimizes downtime during peak loads. Avoid relying solely on UserData scripts or manual deployment methods, as they increase provisioning time and complexity.

- EC2 UserData scripts are scripts or commands that are provided to an Amazon EC2 instance at the time of launch. These scripts are executed by the instance during its first boot to automate tasks such as installing software, configuring the instance, or starting specific applications.

### 4. Scaling and Optimizing Kinesis for Increased Traffic

- If your Kinesis stream encounters throughput exceptions, reshard your stream to scale capacity and reduce the frequency or size of your requests to optimize producer behavior. These strategies address write-side limitations effectively and help your stream handle increased traffic efficiently.

- Resharding your stream to scale capacity means increasing the number of shards in your Kinesis stream to handle higher data throughput by distributing the load across more shards

- Reducing the frequency or size of your requests optimizes producer behavior by sending smaller or fewer data chunks, minimizing the risk of exceeding the stream's write limits.

### 5. Efficient Log Analysis with CloudWatch Logs Insights

- For analyzing recent log data in CloudWatch Logs and identifying the most expensive Lambda requests, use CloudWatch Logs Insights. It provides a simple, interactive querying experience directly in the CloudWatch console

- CloudWatch Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. It automatically discovers fields in logs from AWS services, such as Route 53, Lambda, CloudTrail, VPC Flow Logs, and any application or custom log that emits log events as JSON.

### 6. Optimizing DynamoDB Scans During Heavy Loads

- Run parallel scans to process table segments concurrently, significantly reducing the total scan time.
- Reduce the page size to improve request processing times and manage read capacity unit usage more effectively. These methods directly align with AWS's best practices for handling high scan loads.
- DynamoDB uses eventually consistent reads by default. Eventually consistent reads allow you to read data quickly, but it may not reflect the most recent updates immediately, syncing to the latest version over time.

### 7. Automating Notifications for Build Failures in Continuous Integration

- To set up continuous integration with CodeBuild and ensure the DevOps team is notified of build failures:
  - Store the source code in CodeCommit to integrate the latest code seamlessly into the build process. CodeCommit is a supported source code repository for CodeBuild.
  - Use Amazon EventBridge to capture build events and integrate it with Amazon SNS to send SMS notifications to the team. This ensures timely alerts and efficient monitoring of build processes.

### 8. Troubleshooting API Gateway Timeouts

- IntegrationLatency metric measures the time taken by API Gateway to send the request to the backend Lambda function(for example) and receive a response.
- Latency metric measures the total time taken by API Gateway to process a request, including both the request processing time and the backend integration time. A high latency value indicates that the overall API request-response cycle is taking too long, potentially causing the timeouts.
- When troubleshooting API Gateway timeouts, check the IntegrationLatency metric to identify delays in communicating with the backend Lambda function and the Latency metric to evaluate the overall request processing time.

### 9. DAX for High Read Traffic Optimization

- When experiencing high read traffic in DynamoDB, configuring a DAX cluster is the most effective and immediate solution. It enhances performance by reducing latency through caching, ensuring smooth operation during high-demand periods without requiring structural changes to the table.

### 10. ProvisionedThroughputExceededException error

- The ProvisionedThroughputExceededException error in DynamoDB occurs when the number of requests to a table or a global secondary index exceeds the provisioned read or write capacity. This error is a signal that the demand for the table is higher than the capacity allocated for it.

### 11. Handling UnprocessedKeys in DynamoDB BatchGetItem Responses

- To handle unprocessed keys in BatchGetItem responses, leverage the AWS SDK for JavaScript (or other SDKs) to automatically implement retries with exponential backoff. This ensures efficient error handling and maximizes the success rate of batch operations without additional manual intervention. If using custom logic, always implement retries with exponential backoff to handle transient errors effectively.

### 12. Exponential Backoffs

- Exponential backoff is a way to manage retries when something temporarily fails, like a network issue or an overloaded server. Instead of retrying immediately or at fixed intervals, you wait a little longer each time before trying again.

- For example, after the first failure, you might wait 1 second, then 2 seconds after the next failure, then 4 seconds, and so on. This gives the system or service you're trying to reach some time to recover while avoiding adding more strain by retrying too often.

- It's like giving someone extra time to catch their breath after each failed attempt to answer a question. This approach helps reduce congestion and increases the chances that your retries will succeed.

### 13. AWS SDK Exponential Backoffs

- In the context of AWS SDKs for reliable batch operations, exponential backoff is a strategy used to handle situations where some requests fail temporarily, such as when the system is throttled or experiences transient errors. For example, when you use BatchGetItem in DynamoDB, the response may include unprocessed keys if the service couldn't handle some of the requests due to throughput limits or other temporary conditions.

- AWS SDKs automatically retry these unprocessed requests, applying exponential backoff to manage the retries. This means that after each failed attempt, the SDK waits progressively longerâ€”starting with a short delay and doubling it with each subsequent retry. This approach gives DynamoDB time to recover, avoids overloading the service, and ensures that the unprocessed keys are eventually retrieved without manual intervention.

### 14. Using AWS SDKs for Reliable Batch Operations

- To handle unprocessed keys in BatchGetItem responses, leverage the AWS SDK for JavaScript (or other SDKs) to automatically implement retries with exponential backoff. This ensures efficient error handling and maximizes the success rate of batch operations without additional manual intervention.

- If using custom logic, always implement retries with exponential backoff to handle transient errors effectively.

### 15. AWS Serverless Application Repository (SAR)

- The AWS Serverless Application Repository (SAR) is a service where you can find and share ready-made serverless applications. These are pre-built solutions designed to run on AWS services like Lambda, so you don't have to build everything from scratch.

- You can browse a collection of apps for tasks like data processing, monitoring, or machine learning, then quickly deploy them to your AWS account.

### 16. Leveraging Pre-Built Serverless Applications with SAR

- For a company built on AWS serverless architecture looking to accelerate development with pre-built solutions, the AWS Serverless Application Repository (SAR) is the best service. It provides a library of serverless applications, enabling rapid deployment and customization to meet new business requirements.

### 17. ALB access logs

- ALB access logs are detailed records of all the requests handled by your Application Load Balancer (ALB) in AWS. These logs include useful information like the time of the request, the IP address of the client, the requested URL, the response code, and more. They are stored in an Amazon S3 bucket and can help you troubleshoot issues, monitor traffic, and analyze usage patterns.

### 18. Analyzing Request Patterns with ALB Access Logs

- The feature on the Load Balancer that helps analyze incoming requests for latencies and client IP address patterns is the ALB access logs. These logs provide the granular details needed to understand traffic patterns and optimize application performance.

### 19. Handling Periodic Spikes in Kinesis Streams

- When encountering periodic spikes that lead to ProvisionedThroughputExceededException:

  - Implement exponential backoff for retries to handle temporary capacity issues gracefully.
  - Decrease the frequency or size of your requests to avoid overwhelming the shards.

### 20. X-Ray sampling

- X-Ray sampling is a mechanism used to control the amount of data that X-Ray collects to reduce overhead, particularly in high-traffic applications. While it helps manage the volume of tracing data, it does not help with debugging issues related to why the application fails to send data to X-Ray.

### 21. Understanding Debugging Tools for X-Ray Issues

- When an application fails to send data to X-Ray, ensure:

  - The EC2 instance role is correctly configured with the necessary permissions.
  - The EC2 X-Ray Daemon is running and properly configured.
  - Use CloudTrail to monitor API activity for any potential errors.
  - X-Ray sampling does not provide insights for debugging connectivity or configuration issues.

### 22. Auditing SSM Parameter Store Access

- To generate a report for auditing SSM Parameter Store activity:

  - Use AWS CloudTrail, which tracks all API calls to SSM Parameter Store.
  - Configure CloudTrail to store logs in S3 or send them to CloudWatch Logs for further analysis if required.

### 23. Using X-Ray for Cross-Account Distributed Tracing

- AWS X-Ray helps you track and debug issues in applications that span multiple AWS accounts. It collects detailed trace data from different parts of your application, even if they are running in separate accounts.

- This makes it easier to see how requests flow through your entire system, find bottlenecks, and fix errors. With its centralized view, X-Ray gives teams a clear picture of performance across all accounts, making debugging and monitoring distributed applications much simpler.

- For debugging and tracing across multiple AWS accounts, use AWS X-Ray. It provides detailed trace information and centralized visualization, helping teams identify performance issues and errors in distributed applications.

### 24. Optimize API Performance with Caching

- If your API serves frequently requested, daily updated data, enabling API caching in API Gateway is the most straightforward and effective way to improve performance by reducing latency and backend load.

### 25. API Gateway Mapping Templates

- API Gateway Mapping Templates are tools that let you change the way data is sent to or received from your APIs without changing the backend.
- They use a scripting language called Velocity Template Language (VTL) to transform requests or responses. For example, you can reformat data, hide certain fields, or convert it into a different structure before sending it to the client or backend.
- This helps you customize how your API works without modifying your backend logic, making it easier to adapt to different use cases.

### 26. Ensuring Proper Configuration of Health Checks in ALB

- If EC2 instances are marked as unhealthy by an ALB, check:

  - Security Group Rules: Ensure the ALB's security group can communicate with the EC2 instances on the health check port.
  - Health Check Route: Verify the health check path is correctly configured and returns a 2xx status code. A 2xx response is an HTTP status code category that indicates a successful request. It means the client's request was received, understood, and processed correctly by the server. e.g. 200, 201, 202, 204.

### 27. Optimizing AWS Lambda for CPU-Intensive Workloads

- To make AWS Lambda faster for CPU-heavy tasks, you can increase its memory allocation. When you do this, AWS automatically gives the function more CPU power as well, helping it process tasks quicker. Don't waste time adjusting unrelated settings, like the AWS Region or Lambda layers, because these won't directly improve how fast the function runs. Focusing on memory is the most effective way to boost performance for CPU-intensive workloads.

### 28. Backlog per instance metric and Target tracking scaling policy

- The backlog per instance metric shows how many tasks or messages are waiting to be processed by each running ECS instance. It helps measure workload pressure. - When combined with a target tracking scaling policy, you can automatically adjust the number of ECS tasks to keep the backlog at a manageable level. For example, if the backlog grows because of high traffic, more tasks are launched to process it faster, and when traffic slows down, the tasks scale back down. This approach keeps your application responsive while controlling costs.

### 29. Optimizing ECS with SQS for Variable Workloads

- To handle variable traffic spikes and improve order processing performance:

  - Use the backlog per instance metric to monitor the workload.
  - Combine it with a target tracking scaling policy to dynamically scale ECS tasks based on SQS queue depth.

### 30. Understanding EC2 User Data Behavior

- By default, user data scripts run with root privileges, enabling administrative setup tasks during instance initialization.
- These scripts execute only during the first boot cycle.

### 31. Policy Statement

{
"Version": "2012-10-17",
"Statement": [
{
"Sid": "AllowForUseOfThisKey",
"Effect": "Allow",
"Principal": {
"AWS": "arn:aws:iam::111122223333:role/UserRole"
},
"Action": [
"kms:GenerateDataKeyWithoutPlaintext",
"kms:Decrypt"
],
"Resource": "\*"
}
]
}

- a policy statement is a part of a policy document that defines specific permissions. Each statement includes details like:

  - Effect: Whether the action is allowed or denied (in this case, Allow).
  - Principal: Who gets the permissions (here, the UserRole).
  - Action: What actions are allowed (like kms:GenerateDataKeyWithoutPlaintext and kms:Decrypt).
  - Resource: Where the actions are allowed (here, all resources with \*).

- This policy allows the UserRole (a specific role in AWS, i.e arn:aws:iam::111122223333:role/UserRole) to use a KMS key for two tasks: generating encrypted data keys and decrypting data. These permissions apply to all resources, meaning the role can perform these actions wherever the key is used. This setup is commonly used to enable secure data encryption and decryption processes in AWS applications.

### 32. Target Tracking Scaling Policies in EC2 Auto Scaling

- Target Tracking Scaling Policies in EC2 Auto Scaling work like a thermostat for your servers. You set a target, like keeping CPU usage at 50%, and Auto Scaling automatically adjusts the number of servers to stay at that level.
- If usage goes above 50%, it adds more servers; if it goes below, it removes servers. This ensures your application runs smoothly without overusing or underusing resources.

### 33. Target Tracking Metrics

- ASGAverageNetworkOut: This tracks how much data your Auto Scaling Group is sending out (uploading) over the network. It helps you scale servers if the outgoing traffic becomes too high.

- ALBRequestCountPerTarget: This measures how many requests each server (target) behind your Application Load Balancer is handling. If the number of requests per server gets too high, it can add more servers to balance the load.

- ASGAverageCPUUtilization: This measures the average CPU usage across all servers in your Auto Scaling Group. If the CPU usage gets too high (e.g., 80%), it can add more servers to keep things running smoothly.

- ApproximateNumberOfMessagesVisible (NOT Supported): This metric applies to Amazon SQS queues, measuring the number of messages available in a queue for processing. While it is used for scaling consumers of an SQS queue (e.g., Lambda or EC2 instances), it is not supported directly as a Target Tracking Scaling Policy metric in EC2 Auto Scaling groups.

### 34. Internet Gateway

- An Internet Gateway in AWS is like a bridge that connects your private network (inside AWS) to the public internet. It allows resources in your Virtual Private Cloud (VPC), like servers, to send and receive data over the internet. Without an Internet Gateway, your resources can't directly access or be accessed from the internet.

### 35. Route table

- A route table in the instance's subnet is like a map that tells your instance (a virtual server) where to send data. It decides how the instance communicates with other parts of your network or the internet. For example, it might say, "Send local traffic to other servers in the subnet" or "Send internet traffic through the Internet Gateway." It's essential for directing data to the right place.

### 36. Network ACLs (Access Control Lists

- Network ACLs (Access Control Lists) for a subnet are like security guards for your network. They control what kind of data is allowed in or out of the subnet (a section of your network). You can set rules to allow or block specific types of traffic, like saying "Let in web traffic" or "Block unknown requests." These rules help protect your subnet and its resources.

### 37. Conditions for Internet Connectivity via an Internet Gateway

For an EC2 instance to connect to the Internet via an Internet Gateway:

- The route table in the instance's subnet should have a route to an Internet Gateway.
- The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic.

### 38. Lambda function's execution role

- A Lambda function's execution role is like an ID card that gives the function permission to do specific tasks. For example, if your Lambda function needs to read from a database or write to a storage bucket, the execution role tells AWS, "This function is allowed to do that." It's a way to control what the function can and cannot do securely.

- To track and see what your Lambda function is doing using X-Ray, you need to give it permission to share its activity. This is like allowing the function to send updates about what it's doing to X-Ray. You do this by adding a special permission called AWSXRayDaemonWriteAccess to the function's execution role. If you don't do this, the function won't share its activity, and the X-Ray tool will stay empty.

### 39. Using VPC Flow Logs to Troubleshoot Subnet Traffic Issues

- To check if traffic is reaching a subnet in your VPC, you can use VPC Flow Logs because they track IP traffic going to and from the network interfaces of your VPC resources. These logs help you see details like where the traffic is coming from, where it's going, and whether it is accepted or rejected, making them useful for troubleshooting connectivity issues between subnets or resources.

### 40. AWS CLI --dry-run

- The --dry-run option in AWS CLI is like asking, "Can I do this?" without actually doing it. For example, if you want to check if a user can delete a server (EC2 instance), you run the command with --dry-run to test their permissions. It doesn't make any changes, costs nothing, and is a safe way to check what actions the user is allowed to do.

### 41. Rest Api

- A REST API (Representational State Transfer API) is a way for different software systems to communicate over the internet using standard web protocols like HTTP. It allows clients (like apps or websites) to send requests to a server to get, update, create, or delete data.
- REST APIs use simple URLs to define resources and common HTTP methods like GET (read data), POST (create data), PUT (update data), and DELETE (remove data).
- REST APIs with Serverless Architecture mean using REST APIs to interact with backend services that run without needing to manage servers. In serverless setups, you use services like AWS Lambda or Azure Functions, which automatically handle the execution of your code when an API request is made.

### 42. Building REST APIs with Serverless Architecture in AWS

- To adopt a serverless architecture for REST APIs, use API Gateway to expose AWS Lambda functionality. This approach eliminates infrastructure management, ensures scalability, and is cost-efficient, making it the best choice for serverless applications.

### 43. Amazon SQS Scalability

- Amazon SQS queues can store an unlimited number of messages, making them ideal for systems that process large volumes of data.
- However, consider factors such as message retention, size limits, and cost when designing high-throughput applications.

### 44. NACLs and EC2

- When setting up access for an EC2 instance, you need to make sure data can flow both ways. NACLs (Network ACLs) act like Subnet traffic guards and need rules to allow incoming requests and outgoing replies.
- On the other hand, security groups are smarter and automatically allow replies for any incoming traffic you already permitted into the EC2. This setup ensures your EC2 instance can send and receive data properly with clients on the internet.

### 45. Amazon Cognito User Pools

- Amazon Cognito User Pools are fully managed, scalable, and specifically designed for handling user authentication, sign-up, and sign-in processes, reducing the need for custom coding and infrastructure management.

### 46. Auto Scaling Group Replacement of Unhealthy Instances

When an EC2 instance in an Auto Scaling group is marked as unhealthy:

- The ASG terminates the instance and launches a replacement to maintain the desired capacity.
- This ensures high availability and reliability, with minimal manual intervention.

### 47. Improving Latency for Static Content Delivery

To reduce latency for static content on a website:

- Move static content from EC2 instances to Amazon S3 for better scalability and cost-efficiency.
- Use Amazon CloudFront to cache and distribute static content globally, ensuring faster delivery to users worldwide. This combination offloads work from EC2 and optimizes user experience.

### 48. Kinesis ProvisionedThroughputExceeded

- A ProvisionedThroughputExceeded error in Kinesis happens when too much data is sent to the stream too quickly.
- To fix this, you can add more shards to increase the stream's capacity, allowing it to handle more data.
- Also, enable exponential backoff retries in your producer, which makes it wait longer between retries during high traffic, reducing the chance of overwhelming the stream.

### 49. Custom metrics

- Custom metrics are measurements you define to track specific things about your application that AWS doesn't monitor by default. For example, you might want to track how often an API call succeeds or fails, how long a specific process takes, or how many items a user uploads.
- To keep track of errors in a serverless app, you can create custom metrics in CloudWatch to monitor how often an external API succeeds or fails. Set a CloudWatch alarm to alert your team using SNS (like a text or email) if the error rate gets too high. This setup helps you quickly spot and fix problems, making sure your app runs smoothly.

### 50. Handling Throttling in Amazon SES

- When encountering throttling errors in SES, Implement Exponential Backoff to manage retries and distribute the load over time.

### 51. Pre-Signed URLs for Time-Bound Access:

- By making the S3 bucket private, public access is blocked.
- Pre-signed URLs allow you to grant temporary access to objects in a private S3 bucket.
- These URLs are time-bound, meaning the access is only valid for a specific period. Once the time expires, the URL can no longer be used to access the object.

### 52. ValidateService Lifecycle Hook:

- The ValidateService lifecycle event is the final hook in the AWS CodeDeploy deployment process.
- It is used to verify that the deployed application is working as expected and that the deployment was successful.

### 53. Dead-Letter Queue (DLQ):

- A Dead-Letter Queue (DLQ) is a secondary queue that stores messages that could not be successfully processed by the application after a specified number of attempts.
- Configuring a DLQ for the SQS queue ensures that unprocessed or errored messages are isolated for further analysis and troubleshooting without disrupting the main queue.

### 54. VPC Endpoints for DynamoDB:

- VPC endpoints allow Amazon EC2 instances in a VPC to communicate with AWS services like DynamoDB privately without routing traffic through the public internet.
- VPC endpoints use the AWS PrivateLink service to establish private connectivity within the AWS network.

### 55. Provisioned Concurrency

- Provisioned Concurrency in AWS Lambda keeps a specific number of function instances ready to handle requests at all times. Normally, when a Lambda function is called, there's a small delay (called a cold start) if the function needs to be initialized first.
- With Provisioned Concurrency, the function is pre-warmed, so it can handle incoming requests immediately, without this delay. This is especially useful for applications with high or unpredictable traffic, where fast response times are critical.

### 56. Lambda Auto Scaling and Provisioned Concurrency

- To keep Lambda functions fast during busy times, like a big sale, you can use Provisioned Concurrency, which pre-starts Lambda instances so they're ready to handle requests without delay.
- You can also set up Auto Scaling to adjust the number of pre-started instances automatically based on traffic or a set schedule.
- This reduces delays (cold starts) and ensures your app runs smoothly even when many people use it at once.

### 57. Improving Deployment Performance with AWS CodePipeline

- To make Elastic Beanstalk deployments faster, package all the needed dependencies during the build stage in AWS CodeBuild.
- This way, the application is fully prepared before deployment, and the target instances don't have to download or process dependencies during deployment.
- This method simplifies the process and speeds up deployments, especially when dealing with many instances.

### 58. Avoiding Log Loops in S3 Access Logging

- When you turn on access logging for an S3 bucket, make sure the logs are saved in a different bucket. If you save the logs in the same bucket you're logging, it can create a loop where new logs are created for every logging event, causing endless growth.
- Using a separate bucket for logs prevents this problem and keeps things organized.

### 59. Cloudformation Parameters

- Parameters in AWS CloudFormation are placeholders that allow you to input custom values when creating or updating a stack.
- They make templates more flexible and reusable by letting you define values like instance types, key pair names, or VPC IDs at runtime instead of hardcoding them in the template.
- Parameters can have types (like String, Number, or AWS-specific types)

### 60. Valid CloudFormation Parameter Types:

- String: A general-purpose parameter type for text-based input. It is the most commonly used parameter type.
- CommaDelimitedList: Accepts a comma-separated list of values (e.g., "value1,value2,value3").
- AWS::EC2::KeyPair::KeyName: A predefined AWS-specific parameter type used to validate EC2 key pair names.

### 61. AWS::EC2::KeyPair::KeyName parameter

- The AWS::EC2::KeyPair::KeyName parameter type in AWS CloudFormation is used to reference the name of an existing Amazon EC2 Key Pair. This Key Pair is used for secure access to EC2 instances via SSH.
- When you specify this parameter type in your template, CloudFormation provides a dropdown of available Key Pair names in the region during stack creation, ensuring you select a valid Key Pair.

### 62. ASG Configuration

- To ensure high availability, set your Auto Scaling Group's minimum capacity to at least 2 instances and distribute them across multiple Availability Zones (AZs). - This ensures that if one AZ fails, the application remains online with at least one instance running in another AZ, providing resilience and reducing downtime.

### 63. Enhance Global Website Performance Using Amazon CloudFront

- To reduce latency and improve the user experience for a global audience accessing a static website hosted on S3, Amazon CloudFront is the best solution.
- It leverages edge locations to cache and deliver content quickly and efficiently to users worldwide.

### 64. AWS X-Ray sampling

- AWS X-Ray sampling helps control the amount of trace data sent to X-Ray, which is critical in managing costs when dealing with high request volumes.
- By sampling, you can capture a percentage of the requests to identify trends and troubleshoot issues without collecting data from every single request.
- This reduces the data sent to X-Ray while maintaining sufficient insights into application behavior.

### 65. De-registering an ECS instance

- To de-register an ECS instance, you remove it from the cluster so it no longer runs tasks or services.
- You should deregister the ECS instance before terminating it. This ensures that ECS removes the instance from the cluster properly and stops managing tasks on it. If you terminate the instance first, ECS might still consider it part of the cluster for a while, leading to potential issues or errors.

### 66. Properly De-register ECS Instances to Prevent Stale Resources

- When a container instance in ECS is terminated while in the STOPPED state, the ECS agent running on the instance cannot properly de-register the instance from the ECS cluster. This results in the instance appearing as a stale resource in the ECS cluster instead of being terminated
- When terminating ECS container instances, always ensure they are de-registered from the ECS cluster before shutting them down, especially if they are in the STOPPED state.

### 67. Lambda execution context warm start

- The Lambda execution context is the environment where your Lambda function runs. It includes the memory, CPU, and temporary storage your function uses. It also keeps things like environment variables and any setup you do (like database connections or variables) ready.
- If AWS reuses the same environment for multiple requests, your function runs faster because it doesn't need to set up everything again. This is called a "warm start."

### 68. What is the Function Handler in AWS Lambda?

- The function handler is the part of your Lambda code that gets executed whenever the Lambda function is invoked. It is the main entry point where your logic runs and processes the incoming event.

### 69. What does it mean to "move something out of the function handler"?

- When you "move something out of the function handler," it means initializing or setting up certain resources outside the handler function, so they are created only once, instead of being re-created every time the function runs.

### 70. S3 Client

- An S3 client is a tool in your code that helps you talk to Amazon S3. It's not S3 itself but a helper that lets your application do things like upload, download, or delete files stored in S3.
- For example, if you want to save a file in an S3 bucket, you use the S3 client in your code to send a request to S3, and S3 handles the rest. It's like a messenger that connects your app(or Lambda) to S3.

### 71. Optimize Lambda Performance by Reusing Execution Context

- To improve performance, initialize shared resources (e.g., AWS SDK clients) outside the handler. This leverages Lambda's ability to reuse execution contexts across invocations within the same container.
- Moving S3 client initialization outside the function handler helps the Lambda function run faster because it avoids re-creating the client every time the function is invoked.

### 72. Simplify X-Ray Integration with the X-Ray Daemon

- To enable X-Ray tracing for on-premises servers with minimal configuration, Install and run the X-Ray daemon on the on-premises servers to capture and relay the data to the X-Ray service.

### 73. Custom Metrics for Scaling

- For monitoring or scaling decisions based on unavailable metrics (like RAM usage), you need to create custom metrics and send the data to CloudWatch.
- While detailed monitoring improves the granularity of metrics for CPU, disk, and network, it does not include RAM metrics.

### 74. Choose the Right Environment for Elastic Beanstalk Background Jobs

- Elastic Beanstalk has two types of environments. Web Server Environment is for things like websites or APIs that handle user traffic and can scale with more visitors. Worker Environment is for background tasks, like sending emails or processing files, using a queue to handle jobs. Use Web Server for user-facing apps and Worker for tasks running behind the scenes. This helps make apps fast and flexible!
- Elastic Beanstalk provides a dedicated worker environment for applications that need to process background jobs. This environment type integrates directly with Amazon Simple Queue Service (SQS) to decouple long-running tasks and handle them asynchronously.

### 75. ALB Cross-zone load balancing

- Cross-zone load balancing is always enabled and cannot be disabled for Application Load Balancers, ensuring even traffic distribution across targets in all zones. If traffic imbalances occur with ALBs, investigate other potential causes such as sticky sessions, target health, or scaling configurations.

### 76. ALB Sticky sessions

- Sticky sessions bind a client session to a specific target instance, causing repeated requests from the same client to go to the same instance. This can lead to traffic imbalances, as some instances may receive more traffic if the same client continuously sends requests.

### 77. Instances of a specific capacity type aren't equally distributed across Availability Zones:

- If instances with varying capacity (e.g., higher and lower capacity instances) are deployed in the same load balancer target group, the balancer may favor higher-capacity instances to prevent overwhelming lower-capacity ones. This is a typical behavior for Classic Load Balancers(not ALB) and can result in unequal traffic distribution.
- To avoid uneven distribution, ensure that the instances in the target group have similar capacities or use appropriate configurations, such as enabling weighted target groups

### 78. Importance of Stickiness in Load Balancers

- To maintain session persistence and ensure users don't face re-authentication issues in a multi-instance environment, enable stickiness on your Load Balancer. Stickiness directs all requests from a user to the same backend instance during their session, avoiding issues caused by local session storage.
- If session data must be shared across instances, consider using external storage solutions like Amazon DynamoDB or ElastiCache.

### 79. Importance of Logging for Lambda Troubleshooting

- To monitor and troubleshoot Lambda functions, developers should insert logging statements in the Lambda function code which are then available via CloudWatch logs.

### 80. WebSocket APIs

- WebSocket APIs allow two-way communication between the client and server, so both can send messages anytime. This makes it easy for the server to push updates to users without needing complex polling, keeping connections efficient and responsive.
- WebSocket APIs let the server and client talk to each other in real-time. Both can send messages anytime without waiting, making it great for live updates like chat apps, notifications, or games.

### 81. An HTTP API is a way for apps to talk to a server using simple HTTP requests like GET (to fetch data) or POST (to send data).

- An HTTP API is a way for apps to talk to a server using simple HTTP requests like GET (to fetch data) or POST (to send data).

- A REST API is a type of HTTP API that follows specific rules to make it easier to organize and manage, like using URLs to represent data and HTTP methods to perform actions on it.

- All REST APIs are HTTP APIs because they use HTTP to send and receive data. However, not all HTTP APIs are REST APIs, as REST APIs follow extra rules, like organizing data into resources and using standard HTTP methods (GET, POST, etc.) for actions. Think of REST APIs as a more organized and structured version of HTTP APIs.

### 82. Disaster Recovery Best Practices for Amazon RDS

- To ensure database disaster recovery, leverage cross-Region Read Replicas for failover across regions and enable multi-AZ automated backups for data restoration within the same region.
- Automated backups are restricted to the same region where the database is hosted.

### 83. Leveraging High-Resolution Custom Metrics

- While enabling EC2 detailed monitoring provides metrics every 1 minute, it does not support the 10-second granularity requirement.
- However, _high-resolution custom metric_ captured by your application code or scripts allows for greater precision by capturing data at intervals as low as 1 second, which can then be pushed to CloudWatch using a script or application logic.
- Cloudwatch basic monitoring data is available automatically in a 5-minute interval and detailed monitoring data is available in a 1-minute interval.

### 84. ECS_CLUSTER setting

- The ECS_CLUSTER setting is a configuration in the /etc/ecs/ecs.config file on an EC2 instance that tells the ECS agent which cluster the instance should join. For example, if ECS_CLUSTER=MyCluster, the instance will register with the ECS cluster named MyCluster.
- This setting is important when you want your EC2 instances to be part of a specific ECS cluster, and it must be updated correctly during the instance startup process to ensure proper cluster registration.

### 85. IAM and Resource Policies Work Together

- In AWS, permissions for accessing resources (for example, SQS queues) are determined by the combination of IAM policies and resource-based policies (such as SQS policies). If both policies exist, their permissions are combined.
- However, if there is an explicit deny in either policy, it overrides all allow permissions, ensuring the deny always takes precedence.

### 86. Optimize DynamoDB Scan with Parallel Scans

- To improve the performance of Scan operations on large tables, utilize parallel scans to divide the table into segments and scan them concurrently. This technique allows you to handle large-scale data efficiently when indexing is not an option.

### 87. Efficiently Clear Large DynamoDB Tables

- When you need to delete all items from a large DynamoDB table, the fastest and most cost-efficient way is to delete the entire table and then re-create it.
- Deleting the table removes all the data and its metadata instantly, which is much quicker than deleting items one by one, even in batches.
- This approach also saves costs because you don't consume write capacity units (WCUs) that would be required for individual deletions.
- The most efficient and cost-effective way to delete all records in a large DynamoDB table is to delete and re-create the table. This is faster and uses fewer resources compared to scanning and deleting items one by one, even with batch operations.
- The updated records are typically sourced from an external system, such as a data warehouse, central database, or files in AWS S3, where the master dataset is maintained and updated daily.

### 88. Custom CloudWatch Metrics for Memory Monitoring

- Amazon CloudWatch does not natively collect RAM statistics for EC2 instances. To gather this data, you must run a custom script (such as one written in Python or Shell) on each EC2 instance. This script collects memory usage data and pushes it as a custom metric into CloudWatch.
- A cron job can schedule the script to run at regular intervals, ensuring continuous data collection.

### 89. ElastiCache session data

- In a blue/green deployment setup, user session data must be shared across multiple EC2 instances in the Auto Scaling group to avoid requiring users to log in again after deployments. AWS ElastiCache is designed for caching and session storage, making it an ideal solution for maintaining user session data
- While sticky sessions can temporarily ensure user session persistence by routing a user to the same instance, they are not suitable for scalable environments where instances may frequently change during deployments. Sticky sessions are also prone to session loss when an instance is terminated or replaced.

### 90. Leverage CloudWatch for Error Monitoring and Alarms

- Use CloudWatch Alarms and Metrics to monitor error rates and error rates and creating alarms all function codes.
- Reserve X-Ray for debugging specific performance problems, as it is not designed for monitoring error rates.

### 91. Optimize Content Delivery with Multi-Origin CloudFront Setup

- Amazon CloudFront can be configured with multiple origins, such as an S3 bucket for static content and an Application Load Balancer (ALB) for dynamic content.
- CloudFront ensures optimal content delivery based on user requests, reducing latency for both static and dynamic data across a global user base.
- Using CloudFront with multiple origins is simpler, more efficient, and widely used for serving both static and dynamic content with low latency. It directly integrates with S3 and Application Load Balancer, automatically optimizing content delivery and reducing latency for global users without requiring additional customizations.
- Lambda@Edge is better if you need advanced customization, such as modifying requests/responses, adding security headers, or implementing complex routing logic. It provides more flexibility but requires additional setup and development effort.
- For most scenarios, CloudFront with multiple origins is sufficient and easier to maintain. Use Lambda@Edge only if you have specific customization needs beyond what CloudFront can handle alone.

### 92. Scalable Caching and Geospatial Support with ElastiCache Cluster Mode

- For workloads requiring geospatial capabilities and horizontal scaling, Amazon ElastiCache for Redis with cluster mode enabled is an optimal choice.
- ElastiCache for Redis is a fast, fully managed caching service by AWS. It helps improve app performance by storing data in memory, making it very quick to access.
- It supports features like geospatial data (for location-based queries), real-time messaging, and horizontal scaling for more capacity. It works well with databases like Amazon RDS to speed up queries and handle high traffic efficiently.
- Geospatial data is information about locations on the earth, like coordinates (latitude and longitude). It is used to answer location-based questions, such as finding places nearby, calculating distances, or sorting locations by proximity.
- For example, it helps apps show nearby restaurants, delivery locations, or map directions. Tools like ElastiCache for Redis make these queries fast and efficient.

### 93. Cron Job

- A cron job is a scheduled task that runs automatically at specific times or intervals on a server. It's commonly used for things like running backups, sending emails, or cleaning up files. It's like setting an alarm to make a server do something regularly.

### 94. Kinesis Agent

- The Kinesis Agent is a pre-built, easy-to-install application that automatically collects, processes, and sends log data from EC2 instances to Amazon Kinesis Data Streams.

### 95. Optimize CodeBuild with Dependency Caching

- Caching dependencies on Amazon S3 allows AWS CodeBuild to reuse previously downloaded dependencies across builds. This reduces the time spent resolving dependencies from external repositories like Ivy, which can significantly speed up the build process.

### 96. LongPolling

- When using Amazon SQS, clients often poll the queue for new messages. If there are few messages or empty responses, short polling can result in increased API calls and higher costs. LongPolling reduces these costs by waiting for a message to arrive before returning a response, which minimizes unnecessary empty responses.

### 97. Choose the Right Deployment Strategy for Flexibility and Reliability

- When deploying Java-based applications, Blue/green deployment uses auto scaling group and provides minimal downtime and easy rollback, while In-place deployment allows incremental updates on existing instances. Each strategy is suited to different operational needs.
- Blue/green deployment uses separate environments (e.g., two Auto Scaling Groups) for the current version (blue) and the new version (green). It ensures minimal downtime because traffic can be switched to the new environment once it's verified. Rollbacks are easy since the old environment remains intact.

### 98. Trigger Deployments Automatically with AWS CodePipeline

- Amazon S3 + CodePipeline: Storing source code in an S3 bucket and triggering AWS CodePipeline on updates ensures a quick and simple setup. Any changes to the source files in the S3 bucket will automatically trigger the pipeline for deployment.
- CodeCommit + CodePipeline: Using AWS CodeCommit provides a managed Git repository, and changes pushed to this repository can trigger AWS CodePipeline to build and deploy the application. This approach is commonly used in CI/CD pipelines for version control and automation.

### 99. CodeDeploy rollbacks

- When AWS CodeDeploy is configured with automatic rollbacks, it ensures that if a deployment fails, the last successful deployment is redeployed.
- This process involves initiating a new deployment using the last known working version of the application, with a unique deployment ID.

### 100. CloudFormation Export field

- In AWS CloudFormation, the Export field in the Outputs section of a stack template is used to share specific output values with other stacks.
- By exporting a value, other stacks can use the Fn::ImportValue function to import it, enabling efficient communication and reuse of resources like VPCs, subnets, and security groups between stacks.

### 101. Cluster Mode and Replica Behavior in Redis

- Nodes in the Same Region: ElastiCache for Redis requires all nodes in a cluster to be within the same region for low-latency communication and synchronization. This ensures optimal performance and availability of the cluster.
- No Manual Promotion in Cluster Mode: When Redis is configured with cluster mode enabled, the failover process is automated. Replica nodes cannot be manually promoted to primary because the system manages this process to ensure consistency and availability.

### 102. AWS CloudFormation Deployment Workflow

- To deploy a serverless architecture with AWS CloudFormation CLI commands, first use the `cloudformation package` command to prepare and upload your code and resources to an S3 bucket. Then, use the `cloudformation deploy` command to create or update your stack. This workflow automates and simplifies the deployment of serverless applications.

### 103. Granting Permissions for CI/CD Workflows

- When using AWS CodePipeline and CodeBuild in your CI/CD pipeline, always ensure that the IAM role attached to CodeBuild has sufficient permissions to perform necessary actions, such as uploading artifacts to an S3 bucket. This ensures seamless integration and avoids build failures caused by permission issues.

### 104. Choosing Deployment Policies Based on Environment

- The "All at once" deployment policy is the fastest deployment strategy offered by AWS Elastic Beanstalk. It deploys the updated version to all instances simultaneously. This method is suitable for development and testing environments where speed is prioritized, and downtime is not a concern.

### 105. Simplify Version Management with Lambda Aliases

- To efficiently manage and rollback Lambda function versions with the least operational effort, use Lambda aliases. Aliases act as flexible pointers to function versions, providing a straightforward way to switch between versions while minimizing manual intervention.
- Lambda layers are designed to share common code or dependencies across multiple functions, not to manage versions of a single function.

### 106. AWS::Region pseudo parameter
- The AWS::Region pseudo parameter is a built-in parameter in AWS CloudFormation templates that automatically provides the name of the AWS region where the stack is being deployed.

### 107. Use Annotations for Indexing X-Ray Traces
- Annotations in AWS X-Ray are used to index trace data and allow for efficient searching and filtering. 
- If you want to efficiently search and filter AWS X-Ray traces, always use annotations. They are indexed and allow you to query specific trace attributes for better trace analysis and debugging.

### 108. Use CloudWatch Agent for Centralized Monitoring
- If you want to monitor on-premises servers using Amazon CloudWatch, install the CloudWatch agent and configure it with IAM credentials that have CloudWatch permissions.

### 109. Efficiently manage S3 access for multiple users with policy variables
- If you want to provide personal S3 spaces to many users efficiently, use a single customer-managed policy with policy variables like ${aws:username}. Attach this policy to a group that includes all users.
- Policy variables are placeholders you can use in AWS policies to make them dynamic and reusable. Instead of hardcoding specific values (like a username), you use variables like ${aws:username}. 
- When the policy is applied, AWS automatically replaces the variable with the actual value (e.g., the user's name). This makes it easy to create one policy that works for many users, roles, or resources without needing separate policies for each.

### 110. Use Lazy Loading with TTL for efficient caching in high-traffic scenarios.
- Lazy Loading is a way to load data only when it is needed. Instead of getting everything upfront, it waits until something is requested before retrieving it. 
- TTL (Time-To-Live) is a time limit for how long data should stay in a cache before being removed automatically. Together, they ensure only important data is stored and refreshed when needed, saving time and resources.
- If you use TTL (Time-To-Live) in caching, data is automatically removed after a set period. This ensures that stale or old data doesn't stay in the cache, freeing up space and keeping the cache fresh. 
- Without TTL, data stays in the cache indefinitely until it's manually removed or replaced, which can lead to outdated information being used and the cache becoming bloated, potentially causing slower performance and higher memory usage. 
- If you want to optimize caching for frequently accessed data while controlling costs, use Lazy Loading with TTL. This ensures that only requested data is cached, stale data is evicted automatically, and database load is reduced without unnecessary cache bloat.

### 111. Use Dead Letter Queues to debug failed SQS messages.
- If you want to isolate and troubleshoot messages that fail processing in an SQS queue, always use a Dead Letter Queue. 

### 112. Leverage Auto Scaling for processing large workloads from SQS queues.
- In scenarios where AWS Lambda's 15-minute timeout is a limitation, such as processing large messages or long-running tasks, EC2 instances in an Auto Scaling Group (ASG) replace Lambda by taking over the processing workload. 
- The ASG dynamically adjusts the number of EC2 instances based on the load, making it scalable and cost-efficient. This replacement is necessary when Lambda cannot handle the job due to time or resource constraints. 
- However, in some cases, EC2 with ASG can complement Lambda by offloading specific tasks that Lambda cannot handle, allowing both to work together for better efficiency.

### 113. Enable X-Ray tracing for Fargate tasks with a sidecar container and IAM task roles
- To enable tracing for applications running in Docker containers on AWS Fargate, the X-Ray daemon agent must run alongside the application. 
- This can be achieved by deploying the daemon as a sidecar container, allowing it to receive trace data from the application container. 
- Additionally, the X-Ray container requires the correct IAM task role to access AWS X-Ray for storing and managing trace data.
- The X-Ray daemon in a sidecar container collects trace data from the application running in a Docker container on AWS Fargate. 
- While the daemon gathers and temporarily processes this data, it cannot store or analyze it long-term, which is why it needs to send the data to AWS X-Ray. 
- AWS X-Ray is the service responsible for storing, analyzing, and visualizing the trace data, helping developers monitor and troubleshoot their applications. 
- To enable this, the daemon requires an IAM role that provides the necessary permissions to send data securely to AWS X-Ray. This setup ensures seamless tracing and efficient application performance monitoring.

### 114. Avoid downtime during instance upgrades by using Elastic IPs
- If you want to seamlessly upgrade or replace EC2 instances without affecting the DNS service, assign an Elastic IP to the instance. This ensures that the DNS mapping remains consistent, preventing downtime due to IP address changes.
- An Elastic IP allows seamless upgrades or replacements of EC2 instances without downtime by assigning the same static Elastic IP to the new instance. This ensures uninterrupted access because the DNS record remains linked to the Elastic IP, avoiding any need for changes or propagation delays.
- Using Route 53 without Elastic IP would involve updating the IP address of the new instance in the DNS record, which triggers DNS propagation. This process takes time, making it less reliable for immediate transitions.

### 115. Use RDS MySQL storage auto-scaling to handle rapid growth with minimal effort
- Enabling storage auto-scaling for RDS MySQL ensures that the database can automatically adjust its storage capacity based on demand. This solution addresses the issue of rapidly increasing workloads exceeding the available storage without requiring significant development effort.
- Databases like Amazon RDS MySQL, offer storage auto-scaling to handle growth automatically. When enabled, the database monitors storage usage and automatically increases the allocated storage if it detects that you're running out of space. This ensures the database can handle growing workloads without manual intervention, application changes, or downtime. 

### 116. Ensure Security Group rules allow traffic for load balancer health checks
- When an internet-facing load balancer is used to distribute requests to EC2 instances, the Security Groups of the EC2 instances and the load balancer must allow traffic. Specifically, the Security Group of the EC2 instances should permit incoming traffic from the load balancer's IP range or its Security Group on the port where the application is running (e.g., port 80 for HTTP). 
- If this configuration is missing, the load balancer's health checks will fail, causing timeouts.

### 117. Use CloudWatch Logs and Alarms for tracking and notification on application metrics
- If you want to monitor application performance (like response time) and send notifications for threshold breaches;
  - The first step is to capture the response time metrics from the application by writing them to a log file on the EC2 instance. The Amazon CloudWatch agent can then stream the logs to CloudWatch Logs, where a metric filter can be created to extract the response time metric.
  - The second step is to set up a CloudWatch alarm based on the extracted metric. This alarm will monitor the average response time and trigger an SNS notification when the threshold is exceeded.

### 118. Instance Metadata Query for EC2 Information
- If you want to retrieve EC2 instance-specific details like the instance ID or security group, query the metadata endpoint at http://169.254.169.254/latest/meta-data. This method is simple, lightweight, and does not require additional tools or credentials.
- The IP address 169.254.169.254 is a special address used inside AWS EC2 instances to get information about the instance, like its ID, IP address, or security group. 
- You can use it by running simple commands like curl to ask for details. For example, running curl http://169.254.169.254/latest/meta-data/instance-id will give you the instance ID. This works only from inside the instance and doesn't need internet or extra tools. 
- It's a quick way to find out information about the EC2 instance you're working on.

### 119. AWS_XRAY_DAEMON_ADDRESS
- The AWS_XRAY_DAEMON_ADDRESS environment variable is used by the AWS X-Ray SDK to identify the location of the X-Ray daemon running in an ECS environment. The daemon is responsible for collecting trace data from the application and sending it to the X-Ray service. 
- This variable allows the SDK to properly communicate with the daemon.

### 120. SQS MessageDeduplicationId 
- The MessageDeduplicationId parameter in SQS FIFO queues prevents duplicate messages by acting as a unique ID for each message. If the same ID is sent within a 5-minute deduplication window, SQS recognizes it as a duplicate and rejects it, ensuring the queue receives each message only once during that time.

### 121. Write Through Caching for Real-Time Updates
- If you want to ensure that your cache always serves the latest data for applications requiring up-to-date content, use a Write Through caching strategy. It synchronizes the cache with the database for every write operation, eliminating the risk of stale data.

### 122. Grant API Gateway Permission to Invoke Lambda Functions
- If you want to integrate Lambda with API Gateway using stage variables, update the Lambda function's resource-based policy to grant the API Gateway permission to invoke the function. This ensures seamless interaction between the API Gateway and Lambda, preventing errors due to insufficient permissions.

### 123.Efficient Pagination in AWS CLI
- If you want to paginate results in AWS CLI, use --max-items to set the page size and --starting-token to resume pagination from a specific point. This ensures efficient use of API calls while managing large datasets.
- starting-token is like a bookmark. It tells the system, "Start from here," if you paused or stopped before. This way, it doesnâ€™t show you the same results youâ€™ve already seen.
- max-items limits the number of items returned by the API call, allowing you to control the size of each page of results.

### 124. CloudTrail Event Logging for EBS Volumes
- If you are searching for the creator of an EBS volume, remember:
  - EBS volumes created during an EC2 launch are part of the RunInstances API call and do not have a separate CreateVolume event.
  - Review the RunInstances logs in CloudTrail for details about the creation of associated EBS volumes.

### 125. Using AWS X-Ray for Troubleshooting
- If you need to troubleshoot complex distributed systems like microservices, use AWS X-Ray for detailed tracing and debugging. It helps identify bottlenecks, errors, and issues across API Gateway, backend services, and other components.

### 126. Configuring CloudFront with Origin Failover
- Requests always prioritize the primary origin, and failover to the secondary occurs only during primary origin failure. Once the primary recovers, requests return to it.
- Origin failover requires only two origins (primary and secondary) in an origin group for configuration.
- CloudFront origin failover supports failover only for HTTP methods GET, HEAD, and OPTIONS. Requests using other methods, such as POST or PUT, will not trigger failover to the secondary origin.

### 127. Example of Primary and Secondary Origin in CloudFront Failover
- Primary Origin: An S3 bucket hosting your website's static assets (e.g., mywebsite-primary.s3.amazonaws.com).
- Secondary Origin: Another S3 bucket in a different region (e.g., mywebsite-backup.s3.amazonaws.com) or an HTTP server hosted on EC2 as a fallback for the same assets.
- In this setup: Requests are routed to the primary origin first. If the primary origin fails (e.g., S3 bucket becomes unavailable), CloudFront automatically redirects requests to the secondary origin to ensure uninterrupted service.

### 128. Another Example of Primary and Secondary Origin in CloudFront Failover
- Primary Origin: An Application Load Balancer (ALB) fronting your main web application hosted on EC2 instances (e.g., app-primary.mycompany.com).
- Secondary Origin: A static version of your application hosted in an S3 bucket (e.g., app-backup.s3.amazonaws.com), serving as a fallback if the ALB or EC2 instances fail.
- In this case: Users are directed to the primary origin (ALB). If the ALB or EC2 instances behind it become unavailable, CloudFront automatically redirects traffic to the secondary origin (S3 bucket) to serve static fallback content.

### 129. Log Group and Log Streams
- A log group is a container for storing related logs in Amazon CloudWatch Logs.
- A log stream is a sequence of log events within a log group.
- Each log stream belongs to one log group and represents a single source of logs.
- Log Group is like a folder containing logs for an application or service.
- Log Stream is like a file within the folder, holding logs for a specific instance or session.
- logs:CreateLogGroup: Allows the creation of log groups in CloudWatch Logs.
- logs:CreateLogStream: Allows the creation of log streams within a log group.
- logs:PutLogEvents: Allows writing log events to a specific log stream.
- logs:DescribeLogStreams: Allows retrieving information about existing log streams.

### 130. Configuring ECS to Send Logs to CloudWatch Logs
- To send logs from ECS container instances to CloudWatch Logs, ensure the IAM policy attached to the ECS task or instance role includes these permissions: logs:CreateLogGroup, logs:CreateLogStream, logs:PutLogEvents, and logs:DescribeLogStreams. 
- These actions enable ECS to create and manage log groups and streams, as well as send log data to CloudWatch. Use a wildcard resource (arn:aws:logs:*:*:*) unless access needs to be restricted to specific log groups.

### 131. Using Lambda Authorizer for Custom Authentication
- AWS Lambda Authorizers let you implement custom authentication in API Gateway by running your logic in a Lambda function. This is ideal for scenarios where you need to validate credentials against external data sources like DynamoDB or implement complex custom authentication flows. 
-  Avoid using Amazon Cognito in such cases, as it is designed for managing users within its own system and not for external data store authentication.

### 132. Reducing Latency with CloudFront and Lambda@Edge
- Use Lambda@Edge functions with the origin request event to handle cache misses and dynamically redirect requests to the nearest origin (e.g., S3 bucket).
- Leverage the CloudFront-Viewer-Country header to make routing decisions based on the viewer's geographic location.

### 133. Latency with CloudFront and Lambda@Edge
- Combine CloudFront with Lambda@Edge to reduce latency by customizing request handling. Use a Lambda@Edge function triggered on the origin request event to dynamically route requests to the nearest origin (like an S3 bucket) when a cache miss occurs. 
- Origin request event happens in CloudFront when itâ€™s about to send a request to the main server (origin), like an S3 bucket or web server.
- You can also leverage the CloudFront-Viewer-Country header to make routing decisions based on the viewer's location, ensuring faster and more location-optimized content delivery.

### 134. Example: Reducing Latency with CloudFront and Lambda@Edge
- Imagine you have a global e-commerce website with product images stored in S3 buckets across multiple regions (e.g., us-east-1 and eu-west-1). To reduce latency, you can use CloudFront with a Lambda@Edge function triggered on the origin request event.
- Hereâ€™s how it works:
  - Trigger: When a user requests an image, CloudFront checks its cache. If the image is not cached (cache miss), the origin request event is triggered.
  - Lambda@Edge Function: The function reads the user's location using the CloudFront-Viewer-Country header (e.g., "US" or "FR").
  - Dynamic Routing: Based on the location, the function modifies the request to direct it to the nearest S3 bucket (e.g., US users go to   - us-east-1, EU users go to eu-west-1).
  - Improved Latency: The content is served faster because it comes from the closest origin to the user.

### 135. Avoiding Missing S3 Event Notifications
- In Amazon S3, when two writes are made to the same object in a non-versioned bucket at the same time, the object is overwritten, and only the last write is reflected. As a result, only one event notification is triggered for the last write operation, leading to missing notifications for intermediate writes. 
- This is a known behavior in non-versioned buckets where overwriting an object can lead to skipped event notifications. To avoid this, enable versioning on the bucket, as versioning ensures that each write operation generates a unique version of the object, and an event notification is triggered for each write.

### 136. Managing SQS Message Visibility Timeout
- Use ChangeMessageVisibility to dynamically extend the visibility timeout when message processing takes longer than expected.
- This ensures that no other consumers process the same message prematurely, improving reliability and preventing duplicate work.

### 137. Automating CodeCommit Integration with CloudWatch Event Rules
- Use CloudWatch Event Rules to monitor and respond to CodeCommit changes efficiently.
- EventBridge (previously known as CloudWatch Events) is the recommended tool for setting up triggers based on repository events like pushes, merges, or branch deletions.

### 138. Strong Consistency in Amazon S3
- Amazon S3 now provides strong consistency for all read-after-write and read-after-delete operations. This means:
- Immediate Consistency for Deletes: If an object is deleted, any subsequent read or list operation will reflect the deletion immediately. The deleted object will not return data.
- Eventual Consistency for Bucket Operations: However, bucket-level operations, such as listing all buckets, may still exhibit eventual consistency under certain circumstances. For example, if you delete a bucket and immediately list buckets, the deleted bucket might still appear temporarily.

### 139. Dynamo DB Global Tables for Distributed Users 
- DynamoDB Global Tables replicate data across multiple regions, ensuring that users access the data from the region closest to them. This reduces read and write latencies for globally distributed applications by eliminating the need for cross-region data access.

### 140. Optimizing DynamoDB Performance
- Use Global Tables to reduce latency for applications with users in multiple regions.
- Eventually Consistent Reads: Eventually consistent reads have lower latency compared to strongly consistent reads because they do not guarantee immediate synchronization across replicas. 
- DAX is a caching solution for read-heavy workloads. It does not reduce latency for write-heavy workloads or improve overall table performance.

### 141. Managing the DeleteOnTermination Attribute
- The DeleteOnTermination attribute controls whether the root EBS volume is deleted when the EC2 instance is terminated. This attribute can be modified without stopping the instance. Using the AWS CLI, you can update this flag by running the modify-instance-attribute command to set DeleteOnTermination to False. This ensures that the EBS volume persists even after the instance is terminated.

### 142. Tracking Changes in AWS
- Use AWS CloudTrail to monitor and log API activity in your AWS account, including changes to services like CodePipeline.
- AWS CloudTrail tracks all API activities across your AWS account, not just CodePipeline. This includes changes to services like EC2, S3, Lambda, IAM, and more. It logs actions like creating, updating, or deleting resources, making it a comprehensive tool for monitoring and auditing all AWS service activities in your account.

### 143. Debugging AWS CodeBuild Issues
- Use the AWS CodeBuild Agent locally to debug and troubleshoot BuildSpec files and commands efficiently.
- Running AWS CodeBuild locally using the CodeBuild Agent allows the developer to debug and troubleshoot build issues directly on their local environment. This approach helps identify problems with the BuildSpec file, commands, or configurations without repeatedly triggering builds in the cloud. 
- The CodeBuild Agent emulates the build environment, giving developers an isolated environment to pinpoint issues faster.

### 144. Dead-Letter Queues for Fault Isolation
- A dead-letter queue (DLQ) is a secondary queue that captures messages that could not be successfully processed after the maximum number of processing attempts. 
- By configuring a DLQ for the main SQS queue, you can isolate and store problematic messages for further inspection and troubleshooting. 
- This ensures that such failures do not block the processing of other messages, enabling smoother queue operations.

### 145.  Identifying Client IPs with ALB
- If you want to capture the real client IP behind an ALB, always extract it from the X-Forwarded-For header in your backend. This ensures accurate logging and troubleshooting while maintaining a secure and reliable setup.

### 146. Load Balancer Health Checks
- If you want to prevent traffic from being routed to failed backend instances, always configure health checks on your load balancer. This ensures traffic is sent only to healthy instances, improving availability and reliability.

### 147. ALB Target Group Configuration
- If you want your Application Load Balancer to function correctly, ensure that target groups are assigned and that the targets are healthy. Without targets, the ALB will return HTTP 503 Service Unavailable for incoming requests.
- An HTTP 503 error indicates that the server is currently unavailable. When an Application Load Balancer does not have any targets registered (via target groups), it cannot forward requests, resulting in a 503 Service Unavailable error.
- HTTP 504: Refers to a gateway timeout error when the target server does not respond in time, which does not apply here since there are no targets.

### 148. Fixing Kinesis ProvisionedThroughputExceeded
- ProvisionedThroughputExceeded error happens when too much data is read or written to a single shard in a Kinesis stream. Each shard can handle only a certain amount of data per second. 
- To fix this, you can add more shards to your stream, use better partition keys to spread the data evenly, or slow down retries with backoff to avoid overwhelming the shard.

### 149. Automating Instance Replacement
- If you want ASG to replace unhealthy instances based on ELB health checks, The health check type of your instance's Auto Scaling Group must be changed from EC2 to ELB by using a configuration file.

### 150. What is an ALB Query String?
- An ALB (Application Load Balancer) query string is part of the URL in a request that comes after the ? symbol. 
- It contains key-value pairs that provide additional information to the server, like ?version=v1&user=123. ALB query string rules allow you to route traffic based on these key-value pairs. 
- For example, you can route requests with ?version=v1 to one backend and those with ?version=v2 to another, enabling flexible and dynamic traffic handling.

### 151. Understanding Query String Configuration in ALB Listener Rules
```
[
  {
    "Field": "query-string",
    "QueryStringConfig": {
      "Values": [
        {
          "Key": "version",
          "Value": "v1"
        },
        {
          "Value": "*example*"
        }
      ]
    }
  }
]

```
- This configuration is part of an Application Load Balancer (ALB) listener rule to route traffic based on query string parameters in incoming requests. Here's what it means:

  - Field: "query-string" specifies that routing decisions will be made based on query strings in the request URL.

  - Values:

    - {"Key": "version", "Value": "v1"}: Matches requests where the query string includes version=v1.
    - {"Value": "*example*"}: Matches requests where any part of the query string contains example (wildcard match).
- This rule ensures requests meeting these query string conditions are routed to the specified target group.